中文插件包括两项component：一个简单的用于中文复杂度识别（chinese.gapp）的应用程序和一个叫做“中文分隔器”的component。


为了使用前者，简单地从plugins/Lang-Chinese/resources路径加载应用程序。我们不需要从GATE Developer的插件管理控制台加载这个插件本身。这个应用程序包含用于分隔、词典查找、复杂度识别（通过JAPE语法）和本体共指。这个应用程序利用一些从训练数据自动获取的词典清单（和一个处理他们的语法）+手工制作的词典清单。还有应用程序（listcollector.gapp, adj_collector.gapp和nounperson_collector.gapp）来创建这些清单，其他像共指评价（coreference_eval.gapp）和将输出转换成不同格式（ace-to-muse.gapp）这样执行特殊任务的不同的应用程序。


15.5.1 中文词语分隔


和英文不同，中文的文本没有像空格这样的符号（或是定界符）来明确地将单词从环境中分隔出来。因此，对于自动的中文文本处理，我们可能需要一个用来识别中文单词的系统，这个问题被称作词语分割。在本部分我们描述的插件执行中文词语分割的任务。它基于我们使用的在Sighan 2005用来进行中文词语分割任务的感知器学习算法的工作。我们的基于感知器的系统已经在Sighan-05任务中取得了很好的表现。


这个插件叫做Lang_Chinese并且在GATE分发中可以找到。对应的处理资源的名字是Chinese Segmenter PR。一旦你将PR加载到GATE中，你可以把它加入到流水线应用中。需要注意的是它不能够处理语料库（批量处理文件），而是将文档的路径作为参数（参见下方关于参数的描述）。这个插件可以用于从作为训练数据的已分割中文文本中学习模型，也可以用学习到的模型来分割中文文本。这个插件可以使用不同的学习算法来学习不同的模型。它可以处理不同的中文字符编码，例如UTF-8, GB2312和BIG5。这些选项可以在设置插件运行时参数时选择。


这个插件有五个运行时参数：


l   learningAlg是一个字符串变量，用来标识用于生成模型的学习算法，当前这个变量有两个值：PAUM和SVM，分别代表了两个流行的学习算法Perceptron和SVM。默认值是PAUM。


通常来说，SVM的表现稍好于Perceptron，尤其是在小型训练集上。另一方面，Perceptron的学习要比SVM快得多。因此，如果你使用的是小型的训练集，你可能会想要使用SVM


来获得一个更好的模型。但是，如果你使用的是典型的用于中文单词分割任务的大型训练集，你可能更想使用Perceptron来学习，因为SVM的学习可能会使用很长的时间。另外，在使用大型训练集的时候，Perceptron模型和SVM模型的表现十分相近。https://gate.ac.uk/releases/gate-6.1-build3913-ALL/doc/tao/splitli2.html#XYaoyong05b这个是在中文词语分割上SVM和Perceptron的实验比较。


l   learningMode决定了使用插件的两个模式，一个是从训练数据学习模型，另一个是将已学习到的模型应用于分割中文文本。根据这个情况这个参数有两个值：SEGMENTING和LEARNING。默认值是SEGMENTING，代表着分割中文文本。


需要注意的是，首先需要学习一个模型，然后才可以使用已学习的模型来分割文本。有几个使用Sighan-05 Bakeoff中训练数据的模型在这个插件中可供使用。更多关于提供的模型的描述在下方给出。


l   modelURL指定一个指向包含模型的路径的URL。如果插件处在LEARNING的运行模式，学习到的模型将会被放置到这个路径当中。如果是处在SEGMENTING的运行模式，插件将会使用存储在路径中模型来分割文本。从Sighan-05
bakeoff训练数据学习到的模型将会在下方被讨论。


l   textCode指定了使用的文本的编码方式。例如它可以是UTF-8, BIG5, GB2312或是其他任何用于中文文本的编码方式。需要注意的是，当你使用一个已学习的模型来分割一些中文文本，这些中文文本的编码方式要和用来获得模型的训练文本的编码方式一致。


l   textFilesURL指定了指向包含中文文件路径的URL。所有在这个路径下的文件（不包括那些在这个路径的子路径下的文件）将会被用作输入数据。在LEARNING运行模式下，这些文件将被分割的中文文本包含为训练数据。在SEGMENTING运行模式下，这些文件中的文本将会被分割。被分割的文本將會被存儲到處在叫segmented的子路徑下的相應文件中。


下面的PAUM模型和插件一同被分發並且可以以zip壓縮檔的形式在plugins/Lang_Chinese/resources/models路徑中獲取。請解壓它們來進行使用。這些模型是使用PAUM學習算法學習Sighan-05 bakeoff任務提供的語料庫得到的。


l  
從PKU訓練數據學到的PAUM模型，使用PAUM學習算法和UTF-8編碼方式，存在model-paum-pku-utf8.zip檔案中。


l  
從PKU訓練數據學到的PAUM模型，使用PAUM學習算法和GB2312編碼方式，存在model-paum-pku-gb.zip檔案中。


l  
從AS訓練數據學到的PAUM模型，使用PAUM學習算法和UTF-8編碼方式，存在model-as-utf8.zip檔案中。


l  
從AS訓練數據學到的PAUM模型，使用PAUM學習算法和BIG5編碼方式，存在model-as-big5.zip檔案中。


如我們所見，這些模型是使用不同的訓練數據和不同的中文文本編碼方式學習而來的。PKU訓練數據是中國大陸發佈的新聞文章，使用的是簡體中文；AS訓練數據是台灣地區發佈的新聞文章，使用的是繁體中文。如果你的文本是簡體中文的，你可以使用PKU數據訓練的模型。如果你的文本是繁體中文的，你需要使用AS數據訓練的模型。如果你的數據是GB2312或是任何兼容的編碼方式，你需要使用GB2312編碼方式的語料庫訓練的模型。


需要注意的是，被分割的中文文本（被用作訓練數據或是使用插件生成的）使用空格來將單詞從它的上下文中分隔開來。因此，如果你的數據是像UTF-8這樣的Unicode，你可以使用GATE
Unicode Tokeniser來處理分割后的文本，向文本中添加注釋來表征這些中文單詞。
